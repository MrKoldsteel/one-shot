{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the data and some model musings\n",
    "\n",
    "Let's first load in the data and then get the basic matching nets architecture up and going.  One thing that seems like it could be fun to play with here is adding a siamese kind of constraint to the model on the batch.\n",
    "\n",
    "So when we generate a batch, make sure that we have some same and different images for the test images (balanced).  With the loss for matching nets, we have already placed a loss that passes gradients up to the parameters of the encoder.  But the encoder could also be viewed as part of a larger siamese network and we could add the binary siamese loss to the batch loss as well.  \n",
    "\n",
    "Would this kind of blending of models be useful?  Provide a performance enhancement?\n",
    "\n",
    "On some level, it would at least seem to regularize by demanding the model be proficient at (or adapt to) different tasks.  It would also seem to \"expand\" the dataset a little bit by augmenting the comparisons we make.  Together with traditional data augmentation, how well would this kind of approach work? \n",
    "\n",
    "Let's load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280, 28, 28) (19280, 3) (13180, 28, 28) (13180, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load the background dataset. X_background contains the background\n",
    "# images and y_background class information.\n",
    "train_npz_file = np.load('../data/processed/train_28.npz')\n",
    "X_background, y_background = train_npz_file['arr_0'], train_npz_file['arr_1']\n",
    "\n",
    "# Load the evaluation dataset. X_evaluation contains the background\n",
    "# images and y_evaluation class information.\n",
    "test_npz_file = np.load('../data/processed/test_28.npz')\n",
    "X_evaluation, y_evaluation = test_npz_file['arr_0'], test_npz_file['arr_1']\n",
    "\n",
    "# Put the class information in pandas arrays for ease of exploration\n",
    "# and constrained sampling.  \n",
    "y_background_pd = pd.DataFrame(\n",
    "                    data=y_background, \n",
    "                    columns=['Alphabet', 'Character', 'Drawer']\n",
    "    )\n",
    "y_evaluation_pd = pd.DataFrame(\n",
    "                    data=y_evaluation, \n",
    "                    columns=['Alphabet', 'Character', 'Drawer']\n",
    "    )\n",
    "\n",
    "# Check shapes of images.\n",
    "print(X_background.shape, y_background.shape, X_evaluation.shape, y_evaluation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And split the background set randomly into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further split the background set into training and validation sets\n",
    "# by reserving a fraction of the 20 drawers for validation:\n",
    "# --> first isolate the unique drawers (there are 20 of these)\n",
    "# --> randomly select 16 of these to send to training set and the remaining 4 to validation \n",
    "drawers = y_background_pd.Drawer.unique()\n",
    "trn_drawers = np.random.choice(drawers, 16, replace=False)\n",
    "trn_inds = y_background_pd.Drawer.isin(trn_drawers)\n",
    "X_trn, y_trn = X_background[trn_inds], y_background[trn_inds]\n",
    "X_val, y_val = X_background[~trn_inds], y_background[~trn_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in some custom utilities and create one-shot data loaders for each of the training, validation and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "new_path = os.getcwd() + '/../src'\n",
    "if new_path not in sys.path:\n",
    "    sys.path.append(new_path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data.data_utils import OneShotGenerator\n",
    "from data.data_utils import plot_oneshot_task\n",
    "\n",
    "TrnData = OneShotGenerator(X_trn, y_trn)\n",
    "ValData = OneShotGenerator(X_val, y_val)\n",
    "TstData = OneShotGenerator(X_evaluation, y_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try generating and plotting a one-shot task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a one-shot sample of characters from the Balinese alphabet. The sampled characters are at the same positions in both test and support plots.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 15680 into shape (20,105,105,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d1287b26612e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                         y_pairs[0].Alphabet.iloc[0]) + \\\n\u001b[1;32m      5\u001b[0m       ' The sampled characters are at the same positions in both test and support plots.\\n')\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Note, the plot may fail sometimes since the training data has alphabets with less than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 20 characters.  In this case, the OneShotGenerator samples test and support examples of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 15680 into shape (20,105,105,1)"
     ]
    }
   ],
   "source": [
    "x_pairs, y_pairs = TrnData.generate_one_shot()\n",
    "\n",
    "print('Below is a one-shot sample of characters from the {} alphabet.'.format(\n",
    "                        y_pairs[0].Alphabet.iloc[0]) + \\\n",
    "      ' The sampled characters are at the same positions in both test and support plots.\\n')\n",
    "pairs = [x_pairs[0].reshape(20, 105, 105, 1), x_pairs[1].reshape(20, 105, 105, 1)]\n",
    "# Note, the plot may fail sometimes since the training data has alphabets with less than\n",
    "# 20 characters.  In this case, the OneShotGenerator samples test and support examples of\n",
    "# the entire alphabet (from two randomly selected drawers).  The plot_oneshot (which was\n",
    "# disigned to plot 20-way tasks) fails in this case.  Re-running the cell should generate\n",
    "# a new sample that works.\n",
    "plot_oneshot_task(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating flexible matching networks\n",
    "\n",
    "We want to be able to specify encoder parameters and output/train a matching network.   Maybe sync this up with bayesian optimization so that we can scan over a few different architectures and parameter choices and find something that works well on the data.  \n",
    "\n",
    "Maybe the best place to start is to try to reproduce the architecture from the original paper and see if we can get matching networks matching their results.  Then see if we can improve on this with parameter tuning and maybe a synthesis of methods.\n",
    "\n",
    "One thing we may want to do here is reload and store the data in 28 * 28 format, as this is what's done in the matching nets paper.  So just adjust that script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a159fa20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a710b518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_trn[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_utils import BatchingForMatching\n",
    "\n",
    "TrnData = BatchingForMatching(X_trn, y_trn, cache_size=4)\n",
    "ValData = BatchingForMatching(X_val, y_val, cache_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
