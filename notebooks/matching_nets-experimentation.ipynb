{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the data and some model musings\n",
    "\n",
    "Let's first load in the data and then get the basic matching nets architecture up and going.  One thing that seems like it could be fun to play with here is adding a siamese kind of constraint to the model on the batch.\n",
    "\n",
    "So when we generate a batch, make sure that we have some same and different images for the test images (balanced).  With the loss for matching nets, we have already placed a loss that passes gradients up to the parameters of the encoder.  But the encoder could also be viewed as part of a larger siamese network and we could add the binary siamese loss to the batch loss as well.  \n",
    "\n",
    "Would this kind of blending of models be useful?  Provide a performance enhancement?\n",
    "\n",
    "On some level, it would at least seem to regularize by demanding the model be proficient at (or adapt to) different tasks.  It would also seem to \"expand\" the dataset a little bit by augmenting the comparisons we make.  Together with traditional data augmentation, how well would this kind of approach work? \n",
    "\n",
    "Let's load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280, 28, 28) (19280, 3) (13180, 28, 28) (13180, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load the background dataset. X_background contains the background\n",
    "# images and y_background class information.\n",
    "train_npz_file = np.load('../data/processed/train_28.npz')\n",
    "X_background, y_background = train_npz_file['arr_0'], train_npz_file['arr_1']\n",
    "\n",
    "# Load the evaluation dataset. X_evaluation contains the background\n",
    "# images and y_evaluation class information.\n",
    "test_npz_file = np.load('../data/processed/test_28.npz')\n",
    "X_evaluation, y_evaluation = test_npz_file['arr_0'], test_npz_file['arr_1']\n",
    "\n",
    "# Put the class information in pandas arrays for ease of exploration\n",
    "# and constrained sampling.  \n",
    "y_background_pd = pd.DataFrame(\n",
    "                    data=y_background, \n",
    "                    columns=['Alphabet', 'Character', 'Drawer']\n",
    "    )\n",
    "y_evaluation_pd = pd.DataFrame(\n",
    "                    data=y_evaluation, \n",
    "                    columns=['Alphabet', 'Character', 'Drawer']\n",
    "    )\n",
    "\n",
    "# Check shapes of images.\n",
    "print(X_background.shape, y_background.shape, X_evaluation.shape, y_evaluation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to reshape the image arrays so that they fit with what pytorch is expecting for convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_background = X_background.reshape(X_background.shape[0], 1, 28, 28)\n",
    "X_evaluation = X_evaluation.reshape(X_evaluation.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And split the background set randomly into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further split the background set into training and validation sets\n",
    "# by reserving a fraction of the 20 drawers for validation:\n",
    "# --> first isolate the unique drawers (there are 20 of these)\n",
    "# --> randomly select 16 of these to send to training set and the remaining 4 to validation \n",
    "drawers = y_background_pd.Drawer.unique()\n",
    "trn_drawers = np.random.choice(drawers, 16, replace=False)\n",
    "trn_inds = y_background_pd.Drawer.isin(trn_drawers)\n",
    "X_trn, y_trn = X_background[trn_inds], y_background[trn_inds]\n",
    "X_val, y_val = X_background[~trn_inds], y_background[~trn_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in some custom utilities and create one-shot data loaders for each of the training, validation and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "new_path = os.getcwd() + '/../src'\n",
    "if new_path not in sys.path:\n",
    "    sys.path.append(new_path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data.data_utils import OneShotGenerator\n",
    "from data.data_utils import plot_oneshot_task\n",
    "\n",
    "TstData = OneShotGenerator(X_evaluation, y_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try generating and plotting a one-shot task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a one-shot sample of characters from the Balinese alphabet. The sampled characters are at the same positions in both test and support plots.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 15680 into shape (20,105,105,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d1287b26612e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                         y_pairs[0].Alphabet.iloc[0]) + \\\n\u001b[1;32m      5\u001b[0m       ' The sampled characters are at the same positions in both test and support plots.\\n')\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Note, the plot may fail sometimes since the training data has alphabets with less than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 20 characters.  In this case, the OneShotGenerator samples test and support examples of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 15680 into shape (20,105,105,1)"
     ]
    }
   ],
   "source": [
    "x_pairs, y_pairs = TrnData.generate_one_shot()\n",
    "\n",
    "print('Below is a one-shot sample of characters from the {} alphabet.'.format(\n",
    "                        y_pairs[0].Alphabet.iloc[0]) + \\\n",
    "      ' The sampled characters are at the same positions in both test and support plots.\\n')\n",
    "pairs = [x_pairs[0].reshape(20, 105, 105, 1), x_pairs[1].reshape(20, 105, 105, 1)]\n",
    "# Note, the plot may fail sometimes since the training data has alphabets with less than\n",
    "# 20 characters.  In this case, the OneShotGenerator samples test and support examples of\n",
    "# the entire alphabet (from two randomly selected drawers).  The plot_oneshot (which was\n",
    "# disigned to plot 20-way tasks) fails in this case.  Re-running the cell should generate\n",
    "# a new sample that works.\n",
    "plot_oneshot_task(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating flexible matching networks\n",
    "\n",
    "We want to be able to specify encoder parameters and output/train a matching network.   Maybe sync this up with bayesian optimization so that we can scan over a few different architectures and parameter choices and find something that works well on the data.  \n",
    "\n",
    "Maybe the best place to start is to try to reproduce the architecture from the original paper and see if we can get matching networks matching their results.  Then see if we can improve on this with parameter tuning and maybe a synthesis of methods.\n",
    "\n",
    "One thing we may want to do here is reload and store the data in 28 * 28 format, as this is what's done in the matching nets paper.  So just adjust that script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4df05f0c88>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_trn[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_utils import BatchingForMatching\n",
    "\n",
    "batch_size, im_dim, n_channels, n_way = 128, 28, 1, 20\n",
    "TrnData = BatchingForMatching(X_trn, y_trn, cache_size=48, batch_size=batch_size)\n",
    "ValData = BatchingForMatching(X_val, y_val, cache_size=48, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "cos = nn.CosineSimilarity(dim=2)\n",
    "softmax = torch.nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convLayer(in_planes, out_planes, useDropout = False):\n",
    "    \"3x3 convolution with padding\"\n",
    "    seq = nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True),\n",
    "        nn.BatchNorm2d(out_planes),\n",
    "        nn.ReLU(True),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    )\n",
    "    if useDropout: # Add dropout module\n",
    "        list_seq = list(seq.modules())[1:]\n",
    "        list_seq.append(nn.Dropout(0.1))\n",
    "        seq = nn.Sequential(*list_seq)\n",
    "\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "      \n",
    "encoder = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels=1, \n",
    "                    out_channels=64,\n",
    "                    kernel_size=5\n",
    "                ),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.MaxPool2d(2),\n",
    "                torch.nn.Dropout2d(),\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels=64, \n",
    "                    out_channels=128,\n",
    "                    kernel_size=3\n",
    "                ),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.MaxPool2d(2),\n",
    "                torch.nn.Dropout2d(),\n",
    "                Flatten(),\n",
    "                torch.nn.Linear(73728, 128) #int(128 * 3 * 3 / 4), 128)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "layer_size = 64\n",
    "outsize = (int(math.floor(28 / (2 * 2 * 2 * 2))) ** 2) * layer_size\n",
    "outsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the encoder used in the paper.\n",
    "# Missed that output needs to be n_way,\n",
    "# but don't think that this is the case.\n",
    "encoder = torch.nn.Sequential(\n",
    "                convLayer(1, 64),\n",
    "                convLayer(64, 64),\n",
    "                convLayer(64, 64),\n",
    "                convLayer(64, 64),\n",
    "                Flatten() #,\n",
    "                # nn.Linear(64, n_way)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "encoder = encoder.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model, n_evals=200):\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        target_ims, target_classes, support_set_ims, support_set_classes = loader.generate_batch()\n",
    "      \n",
    "        # Push batch to device\n",
    "        target_ims = target_ims.to(device=device, dtype=dtype)\n",
    "        target_classes = target_classes.to(device=device, dtype=torch.long)\n",
    "        support_set_ims = support_set_ims.to(device=device, dtype=dtype)\n",
    "        support_set_classes = support_set_classes.to(device=device, dtype=dtype)\n",
    "    \n",
    "        # Push batch through the model\n",
    "        target_embedding = model(target_ims)\n",
    "        support_set_embeddings = torch.stack(\n",
    "              [model(support_set_ims[:, i, :, :, :]) for i in range(n_way)]\n",
    "            )\n",
    "\n",
    "        similarities = cos(\n",
    "                  target_embedding.repeat([n_way, 1, 1]), \n",
    "                  support_set_embeddings\n",
    "              ).t()\n",
    "        softmax = torch.nn.Softmax(dim=1)\n",
    "        softmax_sims = softmax(similarities)\n",
    "        preds = softmax_sims.unsqueeze(1).bmm(support_set_classes).squeeze()\n",
    "        \n",
    "        num_correct = 0\n",
    "        for i in range(n_evals):\n",
    "            num_correct += (torch.argmax(preds, dim=1) == target_classes).sum()\n",
    "        acc = float(num_correct) / (batch_size * n_evals)\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, batch_size * n_evals , 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(target_ims, support_set_ims, support_set_classes, model):\n",
    "    # Calculate target and support set embeddings by model\n",
    "    target_embedding = model(target_ims)\n",
    "    support_set_embeddings = torch.stack(\n",
    "        [model(support_set_ims[:, i, :, :, :]) for i in range(n_way)]\n",
    "        )\n",
    "    # calculate similarities, softmax_sims and output predictions\n",
    "    similarities = cos(\n",
    "            target_embedding.repeat([n_way, 1, 1]), \n",
    "            support_set_embeddings\n",
    "        ).t()\n",
    "    softmax_sims = softmax(similarities)\n",
    "    return softmax_sims.unsqueeze(1).bmm(support_set_classes).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50 batches, the training performance is: \n",
      "Got 16800 / 25600 correct (65.62)\n",
      "And the validation performance is: \n",
      "Got 13000 / 25600 correct (50.78)\n",
      "And the loss is: 3.025717740058899\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 100 batches, the training performance is: \n",
      "Got 10600 / 25600 correct (41.41)\n",
      "And the validation performance is: \n",
      "Got 7200 / 25600 correct (28.12)\n",
      "And the loss is: 2.9645908641815186\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 150 batches, the training performance is: \n",
      "Got 7200 / 25600 correct (28.12)\n",
      "And the validation performance is: \n",
      "Got 14600 / 25600 correct (57.03)\n",
      "And the loss is: 2.9602783632278444\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 200 batches, the training performance is: \n",
      "Got 4400 / 25600 correct (17.19)\n",
      "And the validation performance is: \n",
      "Got 8200 / 25600 correct (32.03)\n",
      "And the loss is: 2.959749674797058\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 250 batches, the training performance is: \n",
      "Got 9400 / 25600 correct (36.72)\n",
      "And the validation performance is: \n",
      "Got 25600 / 25600 correct (100.00)\n",
      "And the loss is: 2.959319338798523\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 300 batches, the training performance is: \n",
      "Got 18400 / 25600 correct (71.88)\n",
      "And the validation performance is: \n",
      "Got 24600 / 25600 correct (96.09)\n",
      "And the loss is: 2.9602358198165892\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 350 batches, the training performance is: \n",
      "Got 6000 / 25600 correct (23.44)\n",
      "And the validation performance is: \n",
      "Got 23000 / 25600 correct (89.84)\n",
      "And the loss is: 2.962842335700989\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 400 batches, the training performance is: \n",
      "Got 14400 / 25600 correct (56.25)\n",
      "And the validation performance is: \n",
      "Got 3400 / 25600 correct (13.28)\n",
      "And the loss is: 2.964569101333618\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 450 batches, the training performance is: \n",
      "Got 6400 / 25600 correct (25.00)\n",
      "And the validation performance is: \n",
      "Got 3400 / 25600 correct (13.28)\n",
      "And the loss is: 2.963313889503479\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 500 batches, the training performance is: \n",
      "Got 14000 / 25600 correct (54.69)\n",
      "And the validation performance is: \n",
      "Got 19200 / 25600 correct (75.00)\n",
      "And the loss is: 2.959982533454895\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 550 batches, the training performance is: \n",
      "Got 6200 / 25600 correct (24.22)\n",
      "And the validation performance is: \n",
      "Got 3800 / 25600 correct (14.84)\n",
      "And the loss is: 2.961129832267761\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 600 batches, the training performance is: \n",
      "Got 18400 / 25600 correct (71.88)\n",
      "And the validation performance is: \n",
      "Got 1800 / 25600 correct (7.03)\n",
      "And the loss is: 2.9565726137161255\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 650 batches, the training performance is: \n",
      "Got 7000 / 25600 correct (27.34)\n",
      "And the validation performance is: \n",
      "Got 11200 / 25600 correct (43.75)\n",
      "And the loss is: 2.960881404876709\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 700 batches, the training performance is: \n",
      "Got 10000 / 25600 correct (39.06)\n",
      "And the validation performance is: \n",
      "Got 6000 / 25600 correct (23.44)\n",
      "And the loss is: 2.9599245309829714\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 750 batches, the training performance is: \n",
      "Got 12800 / 25600 correct (50.00)\n",
      "And the validation performance is: \n",
      "Got 4400 / 25600 correct (17.19)\n",
      "And the loss is: 2.9614908409118654\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 800 batches, the training performance is: \n",
      "Got 9200 / 25600 correct (35.94)\n",
      "And the validation performance is: \n",
      "Got 11000 / 25600 correct (42.97)\n",
      "And the loss is: 2.9622169446945192\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 850 batches, the training performance is: \n",
      "Got 10000 / 25600 correct (39.06)\n",
      "And the validation performance is: \n",
      "Got 11800 / 25600 correct (46.09)\n",
      "And the loss is: 2.9589631223678587\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 900 batches, the training performance is: \n",
      "Got 10400 / 25600 correct (40.62)\n",
      "And the validation performance is: \n",
      "Got 3800 / 25600 correct (14.84)\n",
      "And the loss is: 2.957730321884155\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 950 batches, the training performance is: \n",
      "Got 6000 / 25600 correct (23.44)\n",
      "And the validation performance is: \n",
      "Got 5400 / 25600 correct (21.09)\n",
      "And the loss is: 2.9611834049224854\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1000 batches, the training performance is: \n",
      "Got 21000 / 25600 correct (82.03)\n",
      "And the validation performance is: \n",
      "Got 10800 / 25600 correct (42.19)\n",
      "And the loss is: 2.9606253576278685\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1050 batches, the training performance is: \n",
      "Got 19000 / 25600 correct (74.22)\n",
      "And the validation performance is: \n",
      "Got 5200 / 25600 correct (20.31)\n",
      "And the loss is: 2.9602179288864137\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1100 batches, the training performance is: \n",
      "Got 3400 / 25600 correct (13.28)\n",
      "And the validation performance is: \n",
      "Got 12800 / 25600 correct (50.00)\n",
      "And the loss is: 2.959862370491028\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1150 batches, the training performance is: \n",
      "Got 12200 / 25600 correct (47.66)\n",
      "And the validation performance is: \n",
      "Got 6200 / 25600 correct (24.22)\n",
      "And the loss is: 2.9596059322357178\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1200 batches, the training performance is: \n",
      "Got 6000 / 25600 correct (23.44)\n",
      "And the validation performance is: \n",
      "Got 5200 / 25600 correct (20.31)\n",
      "And the loss is: 2.9612391090393064\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1250 batches, the training performance is: \n",
      "Got 5800 / 25600 correct (22.66)\n",
      "And the validation performance is: \n",
      "Got 15000 / 25600 correct (58.59)\n",
      "And the loss is: 2.9603550672531127\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1300 batches, the training performance is: \n",
      "Got 15200 / 25600 correct (59.38)\n",
      "And the validation performance is: \n",
      "Got 19400 / 25600 correct (75.78)\n",
      "And the loss is: 2.9631875276565554\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1350 batches, the training performance is: \n",
      "Got 5600 / 25600 correct (21.88)\n",
      "And the validation performance is: \n",
      "Got 6200 / 25600 correct (24.22)\n",
      "And the loss is: 2.961760401725769\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1400 batches, the training performance is: \n",
      "Got 5000 / 25600 correct (19.53)\n",
      "And the validation performance is: \n",
      "Got 10800 / 25600 correct (42.19)\n",
      "And the loss is: 2.96222403049469\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1450 batches, the training performance is: \n",
      "Got 18800 / 25600 correct (73.44)\n",
      "And the validation performance is: \n",
      "Got 18200 / 25600 correct (71.09)\n",
      "And the loss is: 2.9597745370864867\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1500 batches, the training performance is: \n",
      "Got 12800 / 25600 correct (50.00)\n",
      "And the validation performance is: \n",
      "Got 21800 / 25600 correct (85.16)\n",
      "And the loss is: 2.9619669008255003\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1550 batches, the training performance is: \n",
      "Got 16000 / 25600 correct (62.50)\n",
      "And the validation performance is: \n",
      "Got 4000 / 25600 correct (15.62)\n",
      "And the loss is: 2.963442077636719\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1600 batches, the training performance is: \n",
      "Got 7000 / 25600 correct (27.34)\n",
      "And the validation performance is: \n",
      "Got 6600 / 25600 correct (25.78)\n",
      "And the loss is: 2.9651138401031494\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1650 batches, the training performance is: \n",
      "Got 6000 / 25600 correct (23.44)\n",
      "And the validation performance is: \n",
      "Got 19600 / 25600 correct (76.56)\n",
      "And the loss is: 2.962677478790283\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1700 batches, the training performance is: \n",
      "Got 4000 / 25600 correct (15.62)\n",
      "And the validation performance is: \n",
      "Got 12800 / 25600 correct (50.00)\n",
      "And the loss is: 2.9635904216766358\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1750 batches, the training performance is: \n",
      "Got 8000 / 25600 correct (31.25)\n",
      "And the validation performance is: \n",
      "Got 4200 / 25600 correct (16.41)\n",
      "And the loss is: 2.9612836980819703\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1800 batches, the training performance is: \n",
      "Got 15200 / 25600 correct (59.38)\n",
      "And the validation performance is: \n",
      "Got 17800 / 25600 correct (69.53)\n",
      "And the loss is: 2.958456234931946\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1850 batches, the training performance is: \n",
      "Got 5400 / 25600 correct (21.09)\n",
      "And the validation performance is: \n",
      "Got 5200 / 25600 correct (20.31)\n",
      "And the loss is: 2.954298391342163\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1900 batches, the training performance is: \n",
      "Got 5400 / 25600 correct (21.09)\n",
      "And the validation performance is: \n",
      "Got 15600 / 25600 correct (60.94)\n",
      "And the loss is: 2.963421130180359\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 1950 batches, the training performance is: \n",
      "Got 12000 / 25600 correct (46.88)\n",
      "And the validation performance is: \n",
      "Got 24000 / 25600 correct (93.75)\n",
      "And the loss is: 2.9605697059631346\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2000 batches, the training performance is: \n",
      "Got 13600 / 25600 correct (53.12)\n",
      "And the validation performance is: \n",
      "Got 3200 / 25600 correct (12.50)\n",
      "And the loss is: 2.9603845930099486\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2050 batches, the training performance is: \n",
      "Got 6800 / 25600 correct (26.56)\n",
      "And the validation performance is: \n",
      "Got 6200 / 25600 correct (24.22)\n",
      "And the loss is: 2.9557098627090452\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2100 batches, the training performance is: \n",
      "Got 8800 / 25600 correct (34.38)\n",
      "And the validation performance is: \n",
      "Got 12200 / 25600 correct (47.66)\n",
      "And the loss is: 2.9574357652664185\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2150 batches, the training performance is: \n",
      "Got 2800 / 25600 correct (10.94)\n",
      "And the validation performance is: \n",
      "Got 18800 / 25600 correct (73.44)\n",
      "And the loss is: 2.9607124757766723\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2200 batches, the training performance is: \n",
      "Got 22400 / 25600 correct (87.50)\n",
      "And the validation performance is: \n",
      "Got 11000 / 25600 correct (42.97)\n",
      "And the loss is: 2.9586424922943113\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2250 batches, the training performance is: \n",
      "Got 14800 / 25600 correct (57.81)\n",
      "And the validation performance is: \n",
      "Got 8400 / 25600 correct (32.81)\n",
      "And the loss is: 2.9632836246490477\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2300 batches, the training performance is: \n",
      "Got 6000 / 25600 correct (23.44)\n",
      "And the validation performance is: \n",
      "Got 10800 / 25600 correct (42.19)\n",
      "And the loss is: 2.9634274768829347\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2350 batches, the training performance is: \n",
      "Got 9400 / 25600 correct (36.72)\n",
      "And the validation performance is: \n",
      "Got 2200 / 25600 correct (8.59)\n",
      "And the loss is: 2.961806902885437\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2400 batches, the training performance is: \n",
      "Got 8400 / 25600 correct (32.81)\n",
      "And the validation performance is: \n",
      "Got 4200 / 25600 correct (16.41)\n",
      "And the loss is: 2.9615343618392944\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2450 batches, the training performance is: \n",
      "Got 18000 / 25600 correct (70.31)\n",
      "And the validation performance is: \n",
      "Got 15000 / 25600 correct (58.59)\n",
      "And the loss is: 2.9594354915618895\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2500 batches, the training performance is: \n",
      "Got 6200 / 25600 correct (24.22)\n",
      "And the validation performance is: \n",
      "Got 8000 / 25600 correct (31.25)\n",
      "And the loss is: 2.958532943725586\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2550 batches, the training performance is: \n",
      "Got 5200 / 25600 correct (20.31)\n",
      "And the validation performance is: \n",
      "Got 3800 / 25600 correct (14.84)\n",
      "And the loss is: 2.9641062450408935\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2600 batches, the training performance is: \n",
      "Got 9400 / 25600 correct (36.72)\n",
      "And the validation performance is: \n",
      "Got 1400 / 25600 correct (5.47)\n",
      "And the loss is: 2.960908279418945\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2650 batches, the training performance is: \n",
      "Got 7800 / 25600 correct (30.47)\n",
      "And the validation performance is: \n",
      "Got 13000 / 25600 correct (50.78)\n",
      "And the loss is: 2.9679235792160035\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2700 batches, the training performance is: \n",
      "Got 12000 / 25600 correct (46.88)\n",
      "And the validation performance is: \n",
      "Got 3400 / 25600 correct (13.28)\n",
      "And the loss is: 2.957991247177124\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2750 batches, the training performance is: \n",
      "Got 4000 / 25600 correct (15.62)\n",
      "And the validation performance is: \n",
      "Got 11600 / 25600 correct (45.31)\n",
      "And the loss is: 2.9597931146621703\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2800 batches, the training performance is: \n",
      "Got 15800 / 25600 correct (61.72)\n",
      "And the validation performance is: \n",
      "Got 4000 / 25600 correct (15.62)\n",
      "And the loss is: 2.95947518825531\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2850 batches, the training performance is: \n",
      "Got 15400 / 25600 correct (60.16)\n",
      "And the validation performance is: \n",
      "Got 16000 / 25600 correct (62.50)\n",
      "And the loss is: 2.9656621837615966\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2900 batches, the training performance is: \n",
      "Got 6000 / 25600 correct (23.44)\n",
      "And the validation performance is: \n",
      "Got 4600 / 25600 correct (17.97)\n",
      "And the loss is: 2.960618271827698\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 2950 batches, the training performance is: \n",
      "Got 9200 / 25600 correct (35.94)\n",
      "And the validation performance is: \n",
      "Got 6200 / 25600 correct (24.22)\n",
      "And the loss is: 2.963225693702698\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3000 batches, the training performance is: \n",
      "Got 15800 / 25600 correct (61.72)\n",
      "And the validation performance is: \n",
      "Got 12400 / 25600 correct (48.44)\n",
      "And the loss is: 2.963060426712036\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3050 batches, the training performance is: \n",
      "Got 8000 / 25600 correct (31.25)\n",
      "And the validation performance is: \n",
      "Got 11600 / 25600 correct (45.31)\n",
      "And the loss is: 2.9618134927749633\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3100 batches, the training performance is: \n",
      "Got 3400 / 25600 correct (13.28)\n",
      "And the validation performance is: \n",
      "Got 15800 / 25600 correct (61.72)\n",
      "And the loss is: 2.96964430809021\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3150 batches, the training performance is: \n",
      "Got 3600 / 25600 correct (14.06)\n",
      "And the validation performance is: \n",
      "Got 8600 / 25600 correct (33.59)\n",
      "And the loss is: 2.962844982147217\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3200 batches, the training performance is: \n",
      "Got 7000 / 25600 correct (27.34)\n",
      "And the validation performance is: \n",
      "Got 7600 / 25600 correct (29.69)\n",
      "And the loss is: 2.966505641937256\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3250 batches, the training performance is: \n",
      "Got 9000 / 25600 correct (35.16)\n",
      "And the validation performance is: \n",
      "Got 10000 / 25600 correct (39.06)\n",
      "And the loss is: 2.96661714553833\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3300 batches, the training performance is: \n",
      "Got 12800 / 25600 correct (50.00)\n",
      "And the validation performance is: \n",
      "Got 5800 / 25600 correct (22.66)\n",
      "And the loss is: 2.9640270376205446\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3350 batches, the training performance is: \n",
      "Got 12800 / 25600 correct (50.00)\n",
      "And the validation performance is: \n",
      "Got 1600 / 25600 correct (6.25)\n",
      "And the loss is: 2.959091544151306\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3400 batches, the training performance is: \n",
      "Got 3000 / 25600 correct (11.72)\n",
      "And the validation performance is: \n",
      "Got 2400 / 25600 correct (9.38)\n",
      "And the loss is: 2.962305669784546\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3450 batches, the training performance is: \n",
      "Got 9800 / 25600 correct (38.28)\n",
      "And the validation performance is: \n",
      "Got 16000 / 25600 correct (62.50)\n",
      "And the loss is: 2.958772830963135\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3500 batches, the training performance is: \n",
      "Got 6800 / 25600 correct (26.56)\n",
      "And the validation performance is: \n",
      "Got 7200 / 25600 correct (28.12)\n",
      "And the loss is: 2.96223482131958\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3550 batches, the training performance is: \n",
      "Got 16600 / 25600 correct (64.84)\n",
      "And the validation performance is: \n",
      "Got 1800 / 25600 correct (7.03)\n",
      "And the loss is: 2.960150327682495\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3600 batches, the training performance is: \n",
      "Got 11800 / 25600 correct (46.09)\n",
      "And the validation performance is: \n",
      "Got 9600 / 25600 correct (37.50)\n",
      "And the loss is: 2.9627778053283693\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3650 batches, the training performance is: \n",
      "Got 16600 / 25600 correct (64.84)\n",
      "And the validation performance is: \n",
      "Got 8200 / 25600 correct (32.03)\n",
      "And the loss is: 2.9587488889694216\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3700 batches, the training performance is: \n",
      "Got 14800 / 25600 correct (57.81)\n",
      "And the validation performance is: \n",
      "Got 16800 / 25600 correct (65.62)\n",
      "And the loss is: 2.9573065137863157\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3750 batches, the training performance is: \n",
      "Got 14600 / 25600 correct (57.03)\n",
      "And the validation performance is: \n",
      "Got 14800 / 25600 correct (57.81)\n",
      "And the loss is: 2.9567247104644774\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3800 batches, the training performance is: \n",
      "Got 9800 / 25600 correct (38.28)\n",
      "And the validation performance is: \n",
      "Got 15400 / 25600 correct (60.16)\n",
      "And the loss is: 2.9630842876434325\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3850 batches, the training performance is: \n",
      "Got 19400 / 25600 correct (75.78)\n",
      "And the validation performance is: \n",
      "Got 200 / 25600 correct (0.78)\n",
      "And the loss is: 2.956952338218689\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3900 batches, the training performance is: \n",
      "Got 16400 / 25600 correct (64.06)\n",
      "And the validation performance is: \n",
      "Got 7400 / 25600 correct (28.91)\n",
      "And the loss is: 2.959521107673645\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 3950 batches, the training performance is: \n",
      "Got 5400 / 25600 correct (21.09)\n",
      "And the validation performance is: \n",
      "Got 3600 / 25600 correct (14.06)\n",
      "And the loss is: 2.955984020233154\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4000 batches, the training performance is: \n",
      "Got 3400 / 25600 correct (13.28)\n",
      "And the validation performance is: \n",
      "Got 12800 / 25600 correct (50.00)\n",
      "And the loss is: 2.95735803604126\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4050 batches, the training performance is: \n",
      "Got 10000 / 25600 correct (39.06)\n",
      "And the validation performance is: \n",
      "Got 14600 / 25600 correct (57.03)\n",
      "And the loss is: 2.9610531949996948\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4100 batches, the training performance is: \n",
      "Got 2800 / 25600 correct (10.94)\n",
      "And the validation performance is: \n",
      "Got 13600 / 25600 correct (53.12)\n",
      "And the loss is: 2.9582515239715574\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4150 batches, the training performance is: \n",
      "Got 17000 / 25600 correct (66.41)\n",
      "And the validation performance is: \n",
      "Got 4000 / 25600 correct (15.62)\n",
      "And the loss is: 2.959402947425842\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4200 batches, the training performance is: \n",
      "Got 14400 / 25600 correct (56.25)\n",
      "And the validation performance is: \n",
      "Got 17600 / 25600 correct (68.75)\n",
      "And the loss is: 2.962762508392334\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4250 batches, the training performance is: \n",
      "Got 12200 / 25600 correct (47.66)\n",
      "And the validation performance is: \n",
      "Got 18000 / 25600 correct (70.31)\n",
      "And the loss is: 2.960477523803711\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4300 batches, the training performance is: \n",
      "Got 14400 / 25600 correct (56.25)\n",
      "And the validation performance is: \n",
      "Got 21000 / 25600 correct (82.03)\n",
      "And the loss is: 2.9572435569763185\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4350 batches, the training performance is: \n",
      "Got 11600 / 25600 correct (45.31)\n",
      "And the validation performance is: \n",
      "Got 15800 / 25600 correct (61.72)\n",
      "And the loss is: 2.9587315702438355\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4400 batches, the training performance is: \n",
      "Got 2800 / 25600 correct (10.94)\n",
      "And the validation performance is: \n",
      "Got 3400 / 25600 correct (13.28)\n",
      "And the loss is: 2.959250473976135\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4450 batches, the training performance is: \n",
      "Got 4200 / 25600 correct (16.41)\n",
      "And the validation performance is: \n",
      "Got 200 / 25600 correct (0.78)\n",
      "And the loss is: 2.956194071769714\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4500 batches, the training performance is: \n",
      "Got 13800 / 25600 correct (53.91)\n",
      "And the validation performance is: \n",
      "Got 5600 / 25600 correct (21.88)\n",
      "And the loss is: 2.958455481529236\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4550 batches, the training performance is: \n",
      "Got 16600 / 25600 correct (64.84)\n",
      "And the validation performance is: \n",
      "Got 12000 / 25600 correct (46.88)\n",
      "And the loss is: 2.9588780403137207\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4600 batches, the training performance is: \n",
      "Got 7600 / 25600 correct (29.69)\n",
      "And the validation performance is: \n",
      "Got 15600 / 25600 correct (60.94)\n",
      "And the loss is: 2.955181846618652\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4650 batches, the training performance is: \n",
      "Got 18200 / 25600 correct (71.09)\n",
      "And the validation performance is: \n",
      "Got 11800 / 25600 correct (46.09)\n",
      "And the loss is: 2.9583423137664795\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4700 batches, the training performance is: \n",
      "Got 11600 / 25600 correct (45.31)\n",
      "And the validation performance is: \n",
      "Got 13200 / 25600 correct (51.56)\n",
      "And the loss is: 2.9543013048171995\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4750 batches, the training performance is: \n",
      "Got 19600 / 25600 correct (76.56)\n",
      "And the validation performance is: \n",
      "Got 17200 / 25600 correct (67.19)\n",
      "And the loss is: 2.9608579540252684\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4800 batches, the training performance is: \n",
      "Got 12800 / 25600 correct (50.00)\n",
      "And the validation performance is: \n",
      "Got 9400 / 25600 correct (36.72)\n",
      "And the loss is: 2.9572869777679442\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4850 batches, the training performance is: \n",
      "Got 20200 / 25600 correct (78.91)\n",
      "And the validation performance is: \n",
      "Got 4800 / 25600 correct (18.75)\n",
      "And the loss is: 2.9593003511428835\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4900 batches, the training performance is: \n",
      "Got 10600 / 25600 correct (41.41)\n",
      "And the validation performance is: \n",
      "Got 7400 / 25600 correct (28.91)\n",
      "And the loss is: 2.9554602813720705\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n",
      "After 4950 batches, the training performance is: \n",
      "Got 6600 / 25600 correct (25.78)\n",
      "And the validation performance is: \n",
      "Got 19000 / 25600 correct (74.22)\n",
      "And the loss is: 2.9566509771347045\n",
      "                                                    \n",
      "----------------------------------------------------\n",
      "                                                    \n"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "\n",
    "for i in range(5000):\n",
    "    encoder.train()\n",
    "    # Pull batch from batch generator\n",
    "    target_ims, target_classes, support_set_ims, support_set_classes = TrnData.generate_batch()\n",
    "    # Push to device\n",
    "    # Push batch to device\n",
    "    target_ims = target_ims.to(device=device, dtype=dtype)\n",
    "    target_classes = target_classes.to(device=device, dtype=torch.long)\n",
    "    support_set_ims = support_set_ims.to(device=device, dtype=dtype)\n",
    "    support_set_classes = support_set_classes.to(device=device, dtype=dtype)\n",
    "    \n",
    "    preds = get_preds(target_ims, support_set_ims, support_set_classes, encoder)\n",
    "    lp = loss(preds, target_classes)\n",
    "    optimizer.zero_grad()\n",
    "    lp.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += lp.item()\n",
    "    if (i > 0) and (i % 50 == 0):\n",
    "        print(\"After {} batches, the training performance is: \".format(i))\n",
    "        check_accuracy(TrnData, encoder)\n",
    "        print(\"And the validation performance is: \")\n",
    "        check_accuracy(ValData, encoder)\n",
    "        print(\"And the loss is: {}\".format(running_loss / 50))\n",
    "        running_loss = 0\n",
    "        print(\"                                                    \")\n",
    "        print(\"----------------------------------------------------\")\n",
    "        print(\"                                                    \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems quite finicky to train.  At `1e-5` was bouncing all over the place.  Found one of the implementations that was using learning rate at `1e-3` and weight decay at `1e-4` with Adam...  Hopefully this settles into some good weights without ping-ponging around as much.  With the large initial weights, fluctuations may be large at first but settle down?\n",
    "\n",
    "They also used different batch sizes.  So maybe the thing to do here is pick ranges of:\n",
    "    - learning rate decay\n",
    "    - weight decay\n",
    "    - batch size\n",
    "\n",
    "And maybe a few other things.  Then do a Bayesian optimization over these things to try to find a good configuration. Might want to experiment with similar things with stochastic gradient descent.  \n",
    "\n",
    "Other things to play with here are:\n",
    "    - incorporating a siamese type structure across the batches. E.g. if we do small batches, like 32 we can do a siamese loss over all pairs.  Would this help any.\n",
    "    - could also force the encoder to do other things, like serves as the front end of a decoder.\n",
    "    \n",
    "In both of these instances, we are adding additional losses to backprop derivatives from.  Would this be useful in the form of a sort of regularization.  I.e. providing accountability to more than one metric. \n",
    "\n",
    "Seemed to get similar problems with other  implementations.  Maybe the keys are \n",
    "\n",
    "Would be interesting to watch the errors unfold \n",
    "\n",
    "\n",
    "Another good thing to keep track of is the actual loss we are trying to minimize and what it is doing.  \n",
    "\n",
    "\n",
    "One thought is that it may be ping-ponging around so much because we are sampling 20-shot tasks from the **entire** set of characters.  So some may be **much** more difficult than others.  And because of the caching, this comes in cycles??  Anyway, if this is the case, it's argument for letting the thing train for a long time.  At least until this phenomena stabilizes a bit.  Another thing is that the  tasks on training and validation may be quite a bit more difficult than we will see on testing... How will this affect things?\n",
    "\n",
    "Based on this, may be worth seeing how the loss is doing (which more or less must be a direct reflection of accuracy on the training set/samples... So not sure why this is ping-ponging around so much) on one-shot tasks that reflect what we want to do.  I.e. generated from the validation set from the same alphabet.   \n",
    "\n",
    "And how does learning do if we generate from the same alphabet... Did horribly with siamese nets.\n",
    "\n",
    "Maybe we want to be looking at the actual task on validation data instead of a  mutated, more difficult version of it which may fluctuate super wildly as it seems to be doing.\n",
    "\n",
    "On the one hand, might want to follow their actual method of batch sampling which more closely parallels what we did with the first iteration of siamese nets.  On the other hand, may be worth looking at how Val data is doing on actual one-shot tasks that we care about.  \n",
    "\n",
    "The reference paper  is a bit vague on how to carry out what we're doing.  May want a bit more focus on the support set's in a class... This may be part of the problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "one-shot",
   "language": "python",
   "name": "one-shot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
