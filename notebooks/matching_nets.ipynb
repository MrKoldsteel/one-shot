{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_npz_file = np.load('../data/processed/train.npz')\n",
    "X_train, y_train = train_npz_file['arr_0'], train_npz_file['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19280, 1, 105, 105)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1, 105, 105)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_train_pd = pd.DataFrame(data=y_train, columns=['Alphabet', 'Character', 'Drawer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [np.ones(5) for i in range(3)]\n",
    "np.stack(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [x[0] + '_' + x[1] for x in y_train[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.DataFrame(data=a, columns=['Character'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Gujarati_character42'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.Character.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [x[0] + '_' + x[1] for x in y_train]\n",
    "characters = pd.DataFrame(data=a, columns=['Character'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(964,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters.Character.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_chars = np.random.choice(characters.Character.unique(), 20, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Syriac_(Estrangelo)_character03', 'Burmese_(Myanmar)_character07'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(rnd_chars, 2, replace=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0my_train_pd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Returns a random sample of items from an axis of object.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n : int, optional\n",
       "    Number of items from axis to return. Cannot be used with `frac`.\n",
       "    Default = 1 if `frac` = None.\n",
       "frac : float, optional\n",
       "    Fraction of axis items to return. Cannot be used with `n`.\n",
       "replace : boolean, optional\n",
       "    Sample with or without replacement. Default = False.\n",
       "weights : str or ndarray-like, optional\n",
       "    Default 'None' results in equal probability weighting.\n",
       "    If passed a Series, will align with target object on index. Index\n",
       "    values in weights not found in sampled object will be ignored and\n",
       "    index values in sampled object not in weights will be assigned\n",
       "    weights of zero.\n",
       "    If called on a DataFrame, will accept the name of a column\n",
       "    when axis = 0.\n",
       "    Unless weights are a Series, weights must be same length as axis\n",
       "    being sampled.\n",
       "    If weights do not sum to 1, they will be normalized to sum to 1.\n",
       "    Missing values in the weights column will be treated as zero.\n",
       "    inf and -inf values not allowed.\n",
       "random_state : int or numpy.random.RandomState, optional\n",
       "    Seed for the random number generator (if int), or numpy RandomState\n",
       "    object.\n",
       "axis : int or string, optional\n",
       "    Axis to sample. Accepts axis number or name. Default is stat axis\n",
       "    for given data type (0 for Series and DataFrames, 1 for Panels).\n",
       "\n",
       "Returns\n",
       "-------\n",
       "A new object of same type as caller.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       "Generate an example ``Series`` and ``DataFrame``:\n",
       "\n",
       ">>> s = pd.Series(np.random.randn(50))\n",
       ">>> s.head()\n",
       "0   -0.038497\n",
       "1    1.820773\n",
       "2   -0.972766\n",
       "3   -1.598270\n",
       "4   -1.095526\n",
       "dtype: float64\n",
       ">>> df = pd.DataFrame(np.random.randn(50, 4), columns=list('ABCD'))\n",
       ">>> df.head()\n",
       "          A         B         C         D\n",
       "0  0.016443 -2.318952 -0.566372 -1.028078\n",
       "1 -1.051921  0.438836  0.658280 -0.175797\n",
       "2 -1.243569 -0.364626 -0.215065  0.057736\n",
       "3  1.768216  0.404512 -0.385604 -1.457834\n",
       "4  1.072446 -1.137172  0.314194 -0.046661\n",
       "\n",
       "Next extract a random sample from both of these objects...\n",
       "\n",
       "3 random elements from the ``Series``:\n",
       "\n",
       ">>> s.sample(n=3)\n",
       "27   -0.994689\n",
       "55   -1.049016\n",
       "67   -0.224565\n",
       "dtype: float64\n",
       "\n",
       "And a random 10% of the ``DataFrame`` with replacement:\n",
       "\n",
       ">>> df.sample(frac=0.1, replace=True)\n",
       "           A         B         C         D\n",
       "35  1.981780  0.142106  1.817165 -0.290805\n",
       "49 -1.336199 -0.448634 -0.789640  0.217116\n",
       "40  0.823173 -0.078816  1.009536  1.015108\n",
       "15  1.421154 -0.055301 -1.922594 -0.019696\n",
       "6  -0.148339  0.832938  1.787600 -1.383767\n",
       "\u001b[0;31mFile:\u001b[0m      /anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_pd.sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16693, 16696])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pd[characters.Character == rnd_chars[0]].sample(2).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_drawers = np.random.choice(y_train_pd.Drawer.unique(), 2, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds = y_train_pd[\n",
    "    (y_train_pd.Drawer == rnd_drawers[1]) &\n",
    "    (characters.Character.isin(rnd_chars))\n",
    "].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  467,  1805,  2902,  4334,  4987,  5046,  6332,  6461,  9057,\n",
       "        9125, 11985, 12179, 14255, 14314, 15258, 15426, 15446, 16694,\n",
       "       17497, 18508])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get 19 additional characters that are not `Gujarati_character42`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>Malay_(Jawi_-_Arabic)_character10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16806</th>\n",
       "      <td>Syriac_(Estrangelo)_character23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18259</th>\n",
       "      <td>Bengali_character26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5126</th>\n",
       "      <td>Futurama_character09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11249</th>\n",
       "      <td>Greek_character16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11930</th>\n",
       "      <td>Ojibwe_(Canadian_Aboriginal_Syllabics)_charact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>Korean_character21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19040</th>\n",
       "      <td>Inuktitut_(Canadian_Aboriginal_Syllabics)_char...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5314</th>\n",
       "      <td>N_Ko_character27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12005</th>\n",
       "      <td>Japanese_(katakana)_character45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17677</th>\n",
       "      <td>Cyrillic_character04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12348</th>\n",
       "      <td>Japanese_(katakana)_character03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736</th>\n",
       "      <td>Futurama_character16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6529</th>\n",
       "      <td>Burmese_(Myanmar)_character30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2528</th>\n",
       "      <td>Malay_(Jawi_-_Arabic)_character32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17155</th>\n",
       "      <td>Alphabet_of_the_Magi_character04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13978</th>\n",
       "      <td>Tifinagh_character44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>Gujarati_character14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15097</th>\n",
       "      <td>Asomtavruli_(Georgian)_character17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Character\n",
       "2516                   Malay_(Jawi_-_Arabic)_character10\n",
       "16806                    Syriac_(Estrangelo)_character23\n",
       "18259                                Bengali_character26\n",
       "5126                                Futurama_character09\n",
       "11249                                  Greek_character16\n",
       "11930  Ojibwe_(Canadian_Aboriginal_Syllabics)_charact...\n",
       "1125                                  Korean_character21\n",
       "19040  Inuktitut_(Canadian_Aboriginal_Syllabics)_char...\n",
       "5314                                    N_Ko_character27\n",
       "12005                    Japanese_(katakana)_character45\n",
       "17677                               Cyrillic_character04\n",
       "12348                    Japanese_(katakana)_character03\n",
       "4736                                Futurama_character16\n",
       "6529                       Burmese_(Myanmar)_character30\n",
       "2528                   Malay_(Jawi_-_Arabic)_character32\n",
       "17155                   Alphabet_of_the_Magi_character04\n",
       "13978                               Tifinagh_character44\n",
       "682                                 Gujarati_character14\n",
       "15097                 Asomtavruli_(Georgian)_character17"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters[~(characters.Character == 'Gujarati_character42')].sample(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bengali_character40', 'N_Ko_character02',\n",
       "       'Asomtavruli_(Georgian)_character01', 'Latin_character22',\n",
       "       'Asomtavruli_(Georgian)_character16', 'N_Ko_character28',\n",
       "       'Grantha_character11', 'Balinese_character19',\n",
       "       'Asomtavruli_(Georgian)_character07', 'Latin_character12',\n",
       "       'Tifinagh_character43', 'Asomtavruli_(Georgian)_character21',\n",
       "       'Japanese_(hiragana)_character10', 'Early_Aramaic_character16',\n",
       "       'Mkhedruli_(Georgian)_character01', 'Tifinagh_character20',\n",
       "       'Armenian_character01', 'Malay_(Jawi_-_Arabic)_character28',\n",
       "       'Japanese_(katakana)_character38', 'Grantha_character01'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(characters.Character.unique(), 20, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems there are a couple of ways we could generate batches of one-shot trials for training.  Want them all to be based on caching indices so they are fast and memory efficient.  Here they are:\n",
    "* Just randomly generate a bunch of one-shot tasks by picking characters and two drawers.  Enough of these should do a good job at representing the intricacies of the dataset and comparing a character against a bunch of others.\n",
    "* Go through and generate a bunch of batches for each specific character by: \n",
    "    1. Selecting 19 other characters.\n",
    "    2. Selecting 2 drawers.\n",
    "    3. Find the index numbers for the 21 characters that result from assigning 1 drawer\n",
    "    to the character under consideration and another drawer to both the character under  consideration and the 19 remaining characters.  \n",
    "    4. Track classes appropriately\n",
    "\n",
    "An idea for tracking classes is just to give `np.arange(20)` to the train images in turn and then find the corresponding number for the matching test image.  These can be sent to one-hot at the appropriate time.  \n",
    "\n",
    "For the second method, these batch indices would be stored to a dictionary under the key of the character.  Then to cycle through batches, we could generate a random ordering for unique characters and cycle through the batches in the dictionary items in the same way that we did for the caching of siamese tasks.  \n",
    "\n",
    "This almost seems to be the best way to do it.  That way we make sure that we get a good amount of practice with each character, so the model will see a real variety of images and 20-way one-shot tasks.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_shot_indices(ch, y=y_train_pd, c=characters, bs=32):\n",
    "    \"\"\"\n",
    "    inputs a character and generates a one-shot task for it\n",
    "    by selecting two random drawers and 19 other characters\n",
    "    to form a one-shot task around.  This is all done in terms\n",
    "    of indices.  We can then use this to store a large number\n",
    "    of one-shot tasks for each character.  \n",
    "    \"\"\"\n",
    "    tst_inds, trn_inds = [], []\n",
    "    for _ in range(bs):\n",
    "        rnd_dr = np.random.choice(y.Drawer.unique(),\n",
    "                                  2, replace=False)\n",
    "        rnd_ch = np.random.choice(\n",
    "                    c.Character[c.Character != ch].unique(),\n",
    "                    19, replace=False)\n",
    "        rnd_ch = np.append(ch, rnd_ch)\n",
    "        c_trn_inds = y[(c.Character.isin(rnd_ch)) &\n",
    "                       (y.Drawer == rnd_dr[0])].index.values\n",
    "        c_tst_inds = y[(c.Character == ch) &\n",
    "                       (y.Drawer == rnd_dr[1])].index.values\n",
    "        cls = np.argmax(\n",
    "                c.Character.iloc[c_trn_inds].isin([ch]).values\n",
    "            )\n",
    "        tst_inds.append((c_tst_inds[0], cls))\n",
    "        trn_inds.append(c_trn_inds)\n",
    "        # print(y.iloc[c_tst_inds])\n",
    "        # print(y.iloc[c_trn_inds[0]]) \n",
    "    return np.array(tst_inds), np.array(trn_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BatchingForMatching:\n",
    "    \n",
    "    def __init__(self, X, y, mode='cache', cache_size=20,\n",
    "                       batch_size=32, n_way=20):\n",
    "        self.X = X\n",
    "        self.c = pd.DataFrame(\n",
    "                        data=[ch[0] + '_' + ch[1] for ch in y],\n",
    "                        columns=['Character']\n",
    "                    )\n",
    "        self.unique_characters = np.random.permutation(\n",
    "                                    self.c.Character.unique())\n",
    "        self.num_uc = self.unique_characters.shape[0]\n",
    "        self.batch_num = 0 # keep track of current batch number\n",
    "        self.curr_cache = 0 # and current position in the cache dict \n",
    "        self.y = pd.DataFrame(\n",
    "                        data=y,\n",
    "                        columns=['Alphabet', 'Character', 'Drawer']\n",
    "                    )\n",
    "        self.mode = mode\n",
    "        self.n_way = n_way\n",
    "        self.cache_size = cache_size\n",
    "        self.batch_size = batch_size\n",
    "        self.cache = {}\n",
    "        self.form_cache()\n",
    "    \n",
    "        self.n = X.shape[0]\n",
    "        self.current_task_number = 0\n",
    "        \n",
    "    def get_one_shot_indices(self, ch): #,  y=y_train_pd, c=characters, bs=32):\n",
    "        \"\"\"\n",
    "        inputs a character and generates a one-shot task for it\n",
    "        by selecting two random drawers and 19 other characters\n",
    "        to form a one-shot task around.  This is all done in terms\n",
    "        of indices.  We can then use this to store a large number\n",
    "        of one-shot tasks for each character.  \n",
    "        \"\"\"\n",
    "        tst_inds, trn_inds = [], []\n",
    "        for _ in range(self.batch_size):\n",
    "            rnd_dr = np.random.choice(self.y.Drawer.unique(),\n",
    "                                  2, replace=False)\n",
    "            rnd_ch = np.random.choice(\n",
    "                    self.c.Character[self.c.Character != ch].unique(),\n",
    "                    19, replace=False)\n",
    "            rnd_ch = np.append(ch, rnd_ch)\n",
    "            c_trn_inds = self.y[(self.c.Character.isin(rnd_ch)) &\n",
    "                       (self.y.Drawer == rnd_dr[0])].index.values\n",
    "            c_tst_inds = self.y[(self.c.Character == ch) &\n",
    "                       (self.y.Drawer == rnd_dr[1])].index.values\n",
    "            cls = np.argmax(\n",
    "                self.c.Character.iloc[c_trn_inds].isin([ch]).values\n",
    "                )\n",
    "            tst_inds.append((c_tst_inds[0], cls))\n",
    "            trn_inds.append(c_trn_inds)\n",
    "            # print(y.iloc[c_tst_inds])\n",
    "            # print(y.iloc[c_trn_inds[0]]) \n",
    "        return np.array(tst_inds), np.array(trn_inds)\n",
    "    \n",
    "    def form_cache(self):\n",
    "        for ch in self.unique_characters:\n",
    "            self.cache[ch] = []\n",
    "            for _ in range(self.cache_size):\n",
    "                self.cache[ch].append(self.get_one_shot_indices(ch))\n",
    "                \n",
    "    def generate_batch(self):\n",
    "        \"\"\"\n",
    "        This is more or less giving what we wanted.  Now, the thing\n",
    "        to do is to adjust the output so that it is putting out the\n",
    "        torch tensors that we want for training...  So we need to\n",
    "        form these from what is currently out, and then output them\n",
    "        instead.  Cool beans.  \n",
    "        \"\"\"\n",
    "        out = self.cache[\n",
    "                    self.unique_characters[self.batch_num]\n",
    "                ][self.curr_cache]\n",
    "        self.batch_num = (1 + self.batch_num) % self.num_uc\n",
    "        if self.batch_num == 0:\n",
    "            self.curr_cache = (1 + self.curr_cache) % self.cache_size\n",
    "            \n",
    "        tst_inds, trn_inds = out\n",
    "        \n",
    "        # Construct torch tensors for target images and classes\n",
    "        target_ims = torch.tensor(self.X[tst_inds[:, 0]])\n",
    "        target_classes = torch.tensor(tst_inds[:, 1])\n",
    "        # target_classes = np.zeros((self.batch_size, self.n_way))\n",
    "        # target_classes[np.arange(self.batch_size), tst_inds[:, 1]] = 1\n",
    "        # target_classes = torch.tensor(target_classes, dtype=torch.int32)\n",
    "        \n",
    "        # Construct torch tensors for support set images and classes\n",
    "        support_set_ims = torch.stack(\n",
    "                [torch.tensor(self.X[trn_inds[i, :]]) for i in range(self.batch_size)]\n",
    "            )\n",
    "        support_set_classes = torch.stack(\n",
    "                [torch.eye(self.n_way) for i in range(self.batch_size)]\n",
    "            )\n",
    "        return target_ims, target_classes, support_set_ims, support_set_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 18s, sys: 12.8 s, total: 19min 31s\n",
      "Wall time: 19min 43s\n"
     ]
    }
   ],
   "source": [
    "%time Data = BatchingForMatching(X_train, y_train, cache_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initialization is quite slow.  Another way to do it would be to cache things as we generate them.  So each minibatch we generate is cached until the internal dictionary reaches the cache specs required.  Then, after this, we just reference cache on calling generate_batch()...  Let's try this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, b, c, d) = Test_BatchingForMatching.generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size, im_dim, n_channels, n_way = 32, 105, 1, 20\n",
    "\n",
    "x = torch.randn(batch_size, im_dim, im_dim, n_channels)\n",
    "\n",
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels=1, \n",
    "                    out_channels=64,\n",
    "                    kernel_size=5\n",
    "                ),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.MaxPool2d(2),\n",
    "                torch.nn.Dropout2d(),\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels=64, \n",
    "                    out_channels=128,\n",
    "                    kernel_size=3\n",
    "                ),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.MaxPool2d(2),\n",
    "                torch.nn.Dropout2d(),\n",
    "                Flatten(),\n",
    "                torch.nn.Linear(73728, 128) #int(128 * 3 * 3 / 4), 128)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randn(batch_size, n_channels, im_dim, im_dim)\n",
    "support_set = torch.randn(batch_size, n_way, n_channels, im_dim, im_dim)\n",
    "support_set_classes = torch.randn(batch_size, n_way, n_way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 128]) torch.Size([20, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "repeats = [1 for _ in range(128)]\n",
    "repeats[0] = n_way\n",
    "\n",
    "target_embedding = encoder(target)\n",
    "\n",
    "support_set_embeddings = torch.stack(\n",
    "        [encoder(support_set[:, i, :, :, :]) for i in range(n_way)]\n",
    "    )\n",
    "\n",
    "print(support_set_embeddings.size(), \n",
    "      target_embedding.repeat([n_way, 1, 1]).size())\n",
    "similarities = cos(\n",
    "            target_embedding.repeat([n_way, 1, 1]), \n",
    "            support_set_embeddings\n",
    "  ).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20])\n"
     ]
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "softmax_sims = softmax(similarities)\n",
    "print(softmax_sims.shape)\n",
    "preds = softmax_sims.unsqueeze(1).bmm(support_set_classes).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = torch.rand(batch_size, n_way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.zeros((5, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "tensor(data, dtype=None, device=None, requires_grad=False) -> Tensor\n",
       "\n",
       "Constructs a tensor with :attr:`data`.\n",
       "\n",
       ".. warning::\n",
       "\n",
       "    :func:`torch.tensor` always copies :attr:`data`. If you have a Tensor\n",
       "    ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_`\n",
       "    or :func:`torch.Tensor.detach`.\n",
       "    If you have a NumPy ``ndarray`` and want to avoid a copy, use\n",
       "    :func:`torch.from_numpy`.\n",
       "\n",
       "Args:\n",
       "    data (array_like): Initial data for the tensor. Can be a list, tuple,\n",
       "        NumPy ``ndarray``, scalar, and other types.\n",
       "    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
       "        Default: if ``None``, infers data type from :attr:`data`.\n",
       "    device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
       "        Default: if ``None``, uses the current device for the default tensor type\n",
       "        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
       "        for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
       "    requires_grad (bool, optional): If autograd should record operations on the\n",
       "        returned tensor. Default: ``False``.\n",
       "\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
       "    tensor([[ 0.1000,  1.2000],\n",
       "            [ 2.2000,  3.1000],\n",
       "            [ 4.9000,  5.2000]])\n",
       "\n",
       "    >>> torch.tensor([0, 1])  # Type inference on data\n",
       "    tensor([ 0,  1])\n",
       "\n",
       "    >>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n",
       "                     dtype=torch.float64,\n",
       "                     device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor\n",
       "    tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n",
       "\n",
       "    >>> torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)\n",
       "    tensor(3.1416)\n",
       "\n",
       "    >>> torch.tensor([])  # Create an empty tensor (of size (0,))\n",
       "    tensor([])\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [0, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.eye(2), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19280, 105, 105, 1)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to wrap `flatten` function in a module in order to stack it\n",
    "# in nn.Sequential\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(3 * 32 * 32, hidden_layer_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, 10),\n",
    ")\n",
    "\n",
    "# you can use Nesterov momentum in optim.SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                     momentum=0.9, nesterov=True)\n",
    "\n",
    "train_part34(model, optimizer)\n",
    "\n",
    "\n",
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ims, target_classes, support_set_ims, support_set_classes = Test_BatchingForMatching.generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ims = target_ims.to(device=device, dtype=dtype)\n",
    "target_classes = target_classes.to(device=device, dtype=torch.long)\n",
    "support_set_ims = support_set_ims.to(device=device, dtype=dtype)\n",
    "support_set_classes = support_set_classes.to(device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 128]) torch.Size([20, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "target_embedding = encoder(target_ims)\n",
    "\n",
    "support_set_embeddings = torch.stack(\n",
    "        [encoder(support_set_ims[:, i, :, :, :]) for i in range(n_way)]\n",
    "    )\n",
    "\n",
    "print(support_set_embeddings.size(), target_embedding.repeat([n_way, 1, 1]).size())\n",
    "similarities = cos(\n",
    "            target_embedding.repeat([n_way, 1, 1]), \n",
    "            support_set_embeddings\n",
    "  ).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20])\n"
     ]
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "softmax_sims = softmax(similarities)\n",
    "print(softmax_sims.shape)\n",
    "preds = softmax_sims.unsqueeze(1).bmm(support_set_classes).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = loss(preds, target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "lp.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 18,  1,  8,  5, 16,  1,  2, 13,  2,  0,  1,  2, 15,  6, 19, 13,  6,\n",
       "        15,  2, 12, 10, 13,  9,  0,  7, 11,  8,  1, 15, 16, 15])"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(preds, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 20])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_set_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9943, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # Pull batch from batch generator\n",
    "    target_ims, target_classes, support_set_ims, support_set_classes = Data.generate_batch()\n",
    "    \n",
    "    # Push batch to device\n",
    "    target_ims = target_ims.to(device=device, dtype=dtype)\n",
    "    target_classes = target_classes.to(device=device, dtype=torch.long)\n",
    "    support_set_ims = support_set_ims.to(device=device, dtype=dtype)\n",
    "    support_set_classes = support_set_classes.to(device=device, dtype=dtype)\n",
    "    \n",
    "    # Push batch through the model\n",
    "    target_embedding = encoder(target_ims)\n",
    "    support_set_embeddings = torch.stack(\n",
    "        [encoder(support_set_ims[:, i, :, :, :]) for i in range(n_way)]\n",
    "        )\n",
    "\n",
    "    similarities = cos(\n",
    "            target_embedding.repeat([n_way, 1, 1]), \n",
    "            support_set_embeddings\n",
    "        ).t()\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    softmax_sims = softmax(similarities)\n",
    "    preds = softmax_sims.unsqueeze(1).bmm(support_set_classes).squeeze()\n",
    "    \n",
    "    # Evaluate loss and push gradients back through the graph\n",
    "    lp = loss(preds, target_classes)\n",
    "    optimizer.zero_grad()\n",
    "    lp.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float((torch.argmax(preds, dim=1) == target_classes).sum()) / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Randomly zeroes whole channels of the input tensor.\n",
       "The channels to zero-out are randomized on every forward call.\n",
       "\n",
       "Usually the input comes from :class:`nn.Conv2d` modules.\n",
       "\n",
       "As described in the paper\n",
       "`Efficient Object Localization Using Convolutional Networks`_ ,\n",
       "if adjacent pixels within feature maps are strongly correlated\n",
       "(as is normally the case in early convolution layers) then i.i.d. dropout\n",
       "will not regularize the activations and will otherwise just result\n",
       "in an effective learning rate decrease.\n",
       "\n",
       "In this case, :func:`nn.Dropout2d` will help promote independence between\n",
       "feature maps and should be used instead.\n",
       "\n",
       "Args:\n",
       "    p (float, optional): probability of an element to be zero-ed.\n",
       "    inplace (bool, optional): If set to ``True``, will do this operation\n",
       "        in-place\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, C, H, W)`\n",
       "    - Output: :math:`(N, C, H, W)` (same shape as input)\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> m = nn.Dropout2d(p=0.2)\n",
       "    >>> input = torch.randn(20, 16, 32, 32)\n",
       "    >>> output = m(input)\n",
       "\n",
       ".. _Efficient Object Localization Using Convolutional Networks:\n",
       "   http://arxiv.org/abs/1411.4280\n",
       "\u001b[0;31mFile:\u001b[0m           /anaconda3/lib/python3.6/site-packages/torch/nn/modules/dropout.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.Dropout2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-429-de1e4c71d47f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "encoder.parameters().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9512832"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
